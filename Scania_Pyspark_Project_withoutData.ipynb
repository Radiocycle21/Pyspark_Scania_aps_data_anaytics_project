{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a94136c-d3ba-48d9-8772-84e165261e1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#uploading the test dataset\n",
    "df_test = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"/Volumes/workspace/default/scania_aps_failure_data/aps_failure_test_set.csv\")\n",
    ")\n",
    "\n",
    "df_test.createOrReplaceTempView(\"aps_test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9b86df-c39d-41c7-8153-12d99b0246bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- checking if the test data is loaded correctly\n",
    "\n",
    "select * from aps_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd59e787-fadc-4c5f-851b-d0e5564eb749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "#uploading the training dataset\n",
    "\n",
    "df_training = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"/Volumes/workspace/default/scania_aps_failure_data/aps_failure_training_set.csv\")\n",
    ")\n",
    "\n",
    "df_training.createOrReplaceTempView(\"aps_training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7201bf68-0b96-4c73-82a8-d53f0d818e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- checking if the training data is loaded correctly\n",
    "\n",
    "select * from aps_training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5c4cae-fcb8-4bd0-981a-ca954beab249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- checking the total_rows of both the sets together (Report)\n",
    "select \n",
    "  'Test_set' as Set_type,\n",
    "  count(*) as total_rows \n",
    "from aps_test\n",
    "\n",
    "union all \n",
    "\n",
    "select \n",
    "  'Training_set',\n",
    "  count(*) \n",
    "from aps_training;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49282a79-cf62-4aba-866b-481f9582f9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- combining both the test and training dataset to create large dataset (76K rows)\n",
    "\n",
    "create or replace table aps_all as \n",
    "select * from aps_test\n",
    "union all \n",
    "select * from aps_training; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793fac3b-7fa6-4a0f-a176-d7597cc13d97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# creating the same full dataset in python. the SQL 'aps_all' is 'df' in python.\n",
    "df = spark.table(\"aps_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0aebe45-e604-4747-af4c-cb59be306689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- seeing the full dataset (76K rows)\n",
    "\n",
    "select * from aps_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456582fd-ba62-4a44-b588-55893328b6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- getting the count of the rows of the full dataset \n",
    "\n",
    "select \n",
    "  count(*) as total_rows \n",
    "from aps_all;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda245bf-1749-4a82-b379-c4a4135888d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- getting the count of the columns \n",
    "\n",
    "SELECT COUNT(*) AS column_count\n",
    "FROM information_schema.columns\n",
    "WHERE table_name = 'aps_all';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c0185d-731a-40f6-b88c-bb43588269bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- getting the entire metadata of the columns\n",
    "\n",
    "select *\n",
    "from information_schema.columns\n",
    "where table_name = 'aps_all';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba9893d-5319-4df6-a217-e908511e4c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- getting the datatype of the columns \n",
    "\n",
    "DESCRIBE aps_all;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0dce13c-8d3b-43cd-986d-7f061ed6edaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# found that all the column datatypes are strings. so we cannot do any calculations on this. hence decided to change all the datatypes to int. \n",
    "# first replacing all NA in all the columns as proper nulls \n",
    "\n",
    "# running this to see how many \"bad\" tokens  meaning 'na' exist per column\n",
    "from pyspark.sql.functions import col, trim, lower, when, sum as _sum\n",
    "\n",
    "bad_tokens = ['na','n/a','null','none','nan','']   # tokens we'll treat as missing\n",
    "total_rows = spark.table(\"aps_all\").count()       # or df.count() if you already have df\n",
    "\n",
    "# count occurrences of these tokens per column\n",
    "na_counts = df.select([\n",
    "    _sum(when(lower(trim(col(c))).isin(bad_tokens), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "display(na_counts)   # shows a single row with counts per column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c914cbd7-da83-4a78-a7cf-315516b05ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# in previous step we found the number of 'na' in each column. turns out each of the columns have a lot of 'na'. so now we'll be replacing the 'na' with nulls so that we can change the datatype of the columns to int. \n",
    "\n",
    "from pyspark.sql.functions import trim, lower, when, col\n",
    "\n",
    "bad_tokens = ['na','n/a','null','none','nan','']   # all lowercase here\n",
    "\n",
    "df_clean = df.select([\n",
    "    when(lower(trim(col(c))).isin(bad_tokens), None)\n",
    "        .otherwise(trim(col(c))).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# sanity check\n",
    "df_clean.select(\"aa_000\", \"ab_000\", \"ac_000\").show(20)\n",
    "\n",
    "#df_clean is the new dataset name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51eac82f-aedb-41f5-aa04-7c7da389ded2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#checking the rows to see if there are 'na'\n",
    "\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "check_na = df_clean.select([\n",
    "    _sum(when(col(c) == \"na\", 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean.columns\n",
    "])\n",
    "\n",
    "display(check_na)\n",
    "\n",
    "# found that there are no 'na' as all columns returned 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f5fa21-2798-4945-98bd-cca253dbbf36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#checking the null count to see the count of nulls in each column \n",
    "\n",
    "nulls = df_clean.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean.columns\n",
    "])\n",
    "display(nulls)\n",
    "\n",
    "# showing same as original number of 'na'. so we have successfully replaced 'na' with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29f19bd-7b52-4c8a-b219-163ad7d63af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a new table for the SQL with the cleaned DataFrame\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").saveAsTable(\"aps_all_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b8cfc6-9ba6-41a1-ab22-0b1d3295de76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- checking the dataset to see if the 'na' are replaced by nulls so that we can change the datatype of the dataset columns\n",
    "select * \n",
    "from aps_all_clean;\n",
    "\n",
    "-- succesfully converted all 'na' to nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffda738-9a8c-4508-804f-f75e8791bcf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# converting strings to integers. first we are going to cast all the columns to float and then round them to int.\n",
    "\n",
    "# converting strings to integers safely using long (64-bit) to avoid overflow\n",
    "\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# get all columns except 'class'\n",
    "numeric_cols = df_clean.columns[1:]\n",
    "\n",
    "# convert to float, round, then cast to long; keep 'class' as is\n",
    "df_clean_long = df_clean.select(\n",
    "    col('class'),\n",
    "    *[round(col(c).cast('float')).cast('long').alias(c) for c in numeric_cols]\n",
    ")\n",
    "\n",
    "# save as a new table for step-wise tracking\n",
    "df_clean_long.write.mode(\"overwrite\").saveAsTable(\"aps_all_clean_long\")\n",
    "\n",
    "# df_clean_long is the new table_name\n",
    "# in SQL it is aps_all_clean_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dbcdf61-5c1c-436d-a0f6-94897bc51aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- checking if the new converted dataset is working as expected.\n",
    "select * \n",
    "from aps_all_clean_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8de76c3-a2e2-4a78-93e8-9705c8a4c7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- checking the type conversion. \n",
    "describe aps_all_clean_long;\n",
    "\n",
    "-- successfully changed the type from string to bigint. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12686e25-530a-4ac7-98d8-f66cb33cc646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we are checking the count of nulls for all columns\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "\n",
    "# check null counts for all columns\n",
    "null_counts = df_clean_long.select([\n",
    "    _sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean_long.columns\n",
    "])\n",
    "\n",
    "display(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fd8343-d892-4406-875e-50ceb92ecf09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# finally converting the class column to numeric (pos=1, neg=0)\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# convert class column to numeric (pos=1, neg=0)\n",
    "df_clean_final = df_clean_long.withColumn(\n",
    "    \"class\",\n",
    "    when(col(\"class\") == \"pos\", 1)\n",
    "    .when(col(\"class\") == \"neg\", 0)\n",
    "    .otherwise(None)  # safeguard if anything unexpected\n",
    ")\n",
    "\n",
    "# save as a final clean SQL table\n",
    "df_clean_final.write.mode(\"overwrite\").saveAsTable(\"aps_clean_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628d9203-5108-4c8c-b755-8926f75809f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- checking the conversion from 'pos' and 'neg' to 1 and 0\n",
    "select * from aps_clean_final;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66172663-b3b7-4562-b9de-834cc8dd4c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ Data Cleaning Summary\n",
    "\n",
    "We have completed the data cleaning phase for the APS Failure Dataset. The following steps were performed:\n",
    "\n",
    "1. **Handled missing values**  \n",
    "   - Replaced `\"na\"` strings with proper `null` values.\n",
    "\n",
    "2. **Type conversion**  \n",
    "   - Converted all sensor columns (except `class`) from string → float → bigint for consistency.  \n",
    "   - Stored the result as `df_clean_long`.\n",
    "\n",
    "3. **Target variable cleanup**  \n",
    "   - Converted `class` column:  \n",
    "     - `pos` → `1` (failure)  \n",
    "     - `neg` → `0` (normal)  \n",
    "   - Stored the result as `df_clean_final`.\n",
    "\n",
    "4. **Table versions created for traceability (Python)**  \n",
    "   - `df` → original combined dataset (train + test)  \n",
    "   - `df_clean` → missing-token cleaned (na → NULL)  \n",
    "   - `df_clean_long` → numeric conversion (float → rounded bigint)  \n",
    "   - `df_clean_final` → final cleaned dataset with binary `class`\n",
    "\n",
    "5. **Table versions created for traceability (SQL)**  \n",
    "   - `aps_all` → combined training & test dataset.  \n",
    "   - `aps_all_clean` → NA → null cleaning.  \n",
    "   - `aps_all_clean_long` → numeric conversion.  \n",
    "   - `aps_clean_final` → final cleaned dataset with binary target.\n",
    "\n",
    "---\n",
    "\uD83D\uDC49 The dataset is now fully cleaned and ready for **Exploratory Data Analysis (EDA)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624a66e1-7e37-43d5-82b5-32acfaefb991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# EDA - Exploratory Data Analysis\n",
    "\n",
    "# display the table\n",
    "display(df_clean_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4eb47bc-2f33-4748-92f7-2a49dc896aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### questions for exploratory data analysis at a global level\n",
    "\n",
    "1. What is the total number of rows and columns in df_clean_final?\n",
    "(Shape of the dataset → overall size check)\n",
    "\n",
    "2. How many positive (class=1) vs. negative (class=0) samples are there?\n",
    "(Target variable distribution → class imbalance check)\n",
    "\n",
    "3. How many missing/null values are there in each column, and which columns have the highest % of missing data?\n",
    "(Data completeness overview)\n",
    "\n",
    "4. Which 5 features (columns) have the highest variance, and which 5 have the lowest variance?\n",
    "(Identify informative vs. flat/noisy features)\n",
    "\n",
    "5. Which features are highly correlated with each other (correlation > 0.8 or < -0.8)?\n",
    "(Detect redundancy / multicollinearity across the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04d6acb7-0810-4c95-a57f-aa70f1e16be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. What is the total number of rows and columns in df_clean_final? \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Row count\n",
    "row_count = df_clean_final.count()\n",
    "\n",
    "# Column count\n",
    "col_count = len(df_clean_final.columns)\n",
    "\n",
    "# Create a small summary DataFrame so it's displayed nicely\n",
    "summary_df = spark.createDataFrame(\n",
    "    [(row_count, col_count)],\n",
    "    [\"Total Rows\", \"Total Columns\"]\n",
    ")\n",
    "\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7dbd0b-1cc7-40eb-ace1-f54dd0d4fd9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. How many positive (class=1) vs. negative (class=0) samples are there?\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count of positive (1) vs negative (0) in class column\n",
    "class_distribution = (\n",
    "    df_clean_final.groupBy(\"class\")\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    "    .orderBy(\"class\")\n",
    ")\n",
    "\n",
    "# Add percentage column rounded to 2 decimals\n",
    "total_rows = df_clean_final.count()\n",
    "class_distribution = class_distribution.withColumn(\n",
    "    \"percentage\", F.round(F.col(\"count\") / total_rows * 100, 2)\n",
    ")\n",
    "\n",
    "display(class_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8aca71-afdf-46aa-8625-27fc69f52b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. How many missing/null values are there in each column, and which columns have the highest % of missing data? \n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, count\n",
    "\n",
    "# Total rows for percentage calculation\n",
    "total_rows = df_clean_final.count()\n",
    "\n",
    "# Compute null counts for each column\n",
    "null_counts = df_clean_final.select([\n",
    "    _sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_clean_final.columns\n",
    "]).toPandas().T  # Convert to Pandas to manipulate easily\n",
    "\n",
    "null_counts.columns = [\"null_count\"]\n",
    "null_counts[\"null_percentage\"] = (null_counts[\"null_count\"] / total_rows * 100).round(2)\n",
    "\n",
    "# Sort by highest % missing\n",
    "null_counts_sorted = null_counts.sort_values(\"null_percentage\", ascending=False)\n",
    "null_counts_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaad519-95e4-4179-b3f5-28bdfbf03745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Which 5 features (columns) have the highest variance, and which 5 have the lowest variance?\n",
    "\n",
    "from pyspark.sql.functions import col, variance as _variance\n",
    "\n",
    "# Calculate variance for each numeric column (excluding 'class')\n",
    "numeric_cols = df_clean_final.columns[1:]  # all except 'class'\n",
    "\n",
    "variance_df = df_clean_final.select([\n",
    "    _variance(col(c)).alias(c) for c in numeric_cols\n",
    "]).toPandas().T  # transpose for easier handling\n",
    "\n",
    "variance_df.columns = [\"variance\"]\n",
    "variance_df_sorted = variance_df.sort_values(\"variance\", ascending=False)\n",
    "\n",
    "# Top 5 highest variance\n",
    "top_5 = variance_df_sorted.head(5)\n",
    "\n",
    "# Top 5 lowest variance\n",
    "bottom_5 = variance_df_sorted.tail(5)\n",
    "\n",
    "top_5, bottom_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b182ce8-d664-448e-a47b-60fe7358999f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758355468491}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Which features are highly correlated with each other (correlation > 0.8 or < -0.8)?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert to Pandas for correlation analysis (only numeric features, excluding target)\n",
    "df_pd = df_clean_final.drop(\"class\").toPandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_pd.corr()\n",
    "\n",
    "# Extract upper triangle of correlation matrix (to avoid duplicates)\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "\n",
    "# Find feature pairs with correlations\n",
    "corr_pairs = corr_matrix.where(mask).stack().reset_index()\n",
    "corr_pairs.columns = [\"Feature1\", \"Feature2\", \"Correlation\"]\n",
    "\n",
    "# Add correlation flag\n",
    "corr_pairs[\"High_Correlation\"] = corr_pairs[\"Correlation\"].apply(\n",
    "    lambda x: \"Highly Correlated\" if abs(x) > 0.8 else \"Not Correlated\"\n",
    ")\n",
    "\n",
    "# Filter only strong correlations\n",
    "high_corr_pairs = corr_pairs[corr_pairs[\"High_Correlation\"] == \"Highly Correlated\"] \\\n",
    "    .sort_values(by=\"Correlation\", ascending=False)\n",
    "\n",
    "display(high_corr_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e800d8-a518-45a3-ba1b-b1d4df97b425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### questions for exploratory data analysis at each column level \n",
    "\n",
    "1. For each numeric column, what are the mean, median, and mode values?\n",
    "(Central tendency check)\n",
    "\n",
    "2. For each numeric column, what are the standard deviation, variance, min, max, and range (max−min)?\n",
    "(Data spread and scale check)\n",
    "\n",
    "3. For each numeric column, what is the skewness and kurtosis?\n",
    "(Distribution shape: symmetry & tails)\n",
    "\n",
    "4. For each numeric column, what is the percentage of missing/null values?\n",
    "(Column-level completeness check)\n",
    "\n",
    "5. For each numeric column, are there extreme outliers (e.g., values beyond 1.5×IQR or z-score > 3)?\n",
    "(Outlier detection)\n",
    "\n",
    "6. How does the distribution (histogram) of each sensor column look? Is it normal, uniform, or skewed?\n",
    "(Pattern detection per sensor)\n",
    "\n",
    "7. How does each sensor column differ between class=1 vs. class=0 (group-wise means/medians)?\n",
    "(Feature discrimination check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3276f0d4-c0dd-4fab-be4d-6d9a41d53c7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. For each numeric column, what are the mean, median, and mode values? (Central tendency check)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, mean, expr, count, when, first\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get all numeric columns except the target\n",
    "numeric_cols = df_clean_final.columns[1:]  # skipping 'class'\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "for c in numeric_cols:\n",
    "    # Compute mean\n",
    "    col_mean = df_clean_final.select(mean(col(c)).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "    \n",
    "    # Compute median using approxQuantile\n",
    "    col_median = df_clean_final.approxQuantile(c, [0.5], 0.01)[0]\n",
    "    \n",
    "    # Compute mode: find value with highest frequency\n",
    "    mode_df = df_clean_final.groupBy(c).count().orderBy(F.desc(\"count\"))\n",
    "    col_mode = mode_df.first()[c]\n",
    "    \n",
    "    # Append result\n",
    "    results.append((c, col_mean, col_median, col_mode))\n",
    "\n",
    "# Convert to PySpark DataFrame for display\n",
    "eda_central_tendency = spark.createDataFrame(results, [\"Column\", \"Mean\", \"Median\", \"Mode\"])\n",
    "\n",
    "# Display results\n",
    "display(eda_central_tendency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73131f2c-5214-4aa1-9571-112d1fd5d552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. For each numeric column, what are the standard deviation, variance, min, max, and range (max−min)? (Data spread and scale check)\n",
    "\n",
    "from pyspark.sql.functions import stddev, variance, min, max\n",
    "\n",
    "# List of numeric columns except target\n",
    "numeric_cols = df_clean_final.columns[1:]\n",
    "\n",
    "# Initialize list to store results\n",
    "spread_results = []\n",
    "\n",
    "for c in numeric_cols:\n",
    "    stats = df_clean_final.select(\n",
    "        stddev(col(c)).alias(\"stddev\"),\n",
    "        variance(col(c)).alias(\"variance\"),\n",
    "        min(col(c)).alias(\"min\"),\n",
    "        max(col(c)).alias(\"max\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    col_stddev = stats[\"stddev\"]\n",
    "    col_variance = stats[\"variance\"]\n",
    "    col_min = stats[\"min\"]\n",
    "    col_max = stats[\"max\"]\n",
    "    col_range = col_max - col_min\n",
    "    \n",
    "    spread_results.append((c, col_stddev, col_variance, col_min, col_max, col_range))\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "eda_spread = spark.createDataFrame(\n",
    "    spread_results, [\"Column\", \"StdDev\", \"Variance\", \"Min\", \"Max\", \"Range\"]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display(eda_spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45f77c0-4f1e-4381-9f96-11150f056601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. For each numeric column, what are the skewness and kurtosis values? (Data shape check)\n",
    "\n",
    "from pyspark.sql.functions import skewness, kurtosis, when, col\n",
    "\n",
    "numeric_cols = df_clean_final.columns[1:]\n",
    "\n",
    "shape_results = []\n",
    "\n",
    "for c in numeric_cols:\n",
    "    stats = df_clean_final.select(\n",
    "        skewness(col(c)).alias(\"skewness\"),\n",
    "        kurtosis(col(c)).alias(\"kurtosis\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    col_skewness = stats[\"skewness\"]\n",
    "    col_kurtosis = stats[\"kurtosis\"]\n",
    "    \n",
    "    # Interpret skewness\n",
    "    if col_skewness is None:\n",
    "        skew_desc = \"No data\"\n",
    "    elif col_skewness > 0.5:\n",
    "        skew_desc = \"Positively skewed\"\n",
    "    elif col_skewness < -0.5:\n",
    "        skew_desc = \"Negatively skewed\"\n",
    "    else:\n",
    "        skew_desc = \"Approximately symmetric\"\n",
    "    \n",
    "    # Interpret kurtosis\n",
    "    if col_kurtosis is None:\n",
    "        kurt_desc = \"No data\"\n",
    "    elif col_kurtosis > 3:\n",
    "        kurt_desc = \"Heavy tails / Leptokurtic\"\n",
    "    elif col_kurtosis < 3:\n",
    "        kurt_desc = \"Light tails / Platykurtic\"\n",
    "    else:\n",
    "        kurt_desc = \"Normal tails / Mesokurtic\"\n",
    "    \n",
    "    shape_results.append((c, col_skewness, skew_desc, col_kurtosis, kurt_desc))\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "eda_shape = spark.createDataFrame(\n",
    "    shape_results, [\"Column\", \"Skewness\", \"Skewness_Interpretation\", \"Kurtosis\", \"Kurtosis_Interpretation\"]\n",
    ")\n",
    "\n",
    "display(eda_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8999abb-d098-4483-9a65-2a6f7db78d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. For each numeric column, what is the percentage of missing/null values? (Column-level completeness check)\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "# Total rows for percentage calculation\n",
    "total_rows = df_clean_final.count()\n",
    "\n",
    "# Compute null counts for each column\n",
    "null_counts = df_clean_final.select([\n",
    "    _sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_clean_final.columns[1:]  # Exclude 'class'\n",
    "]).toPandas().T  # Convert to Pandas for easier manipulation\n",
    "\n",
    "null_counts.columns = [\"null_count\"]\n",
    "null_counts[\"null_percentage\"] = (null_counts[\"null_count\"] / total_rows * 100).round(2)\n",
    "\n",
    "# Sort by highest % missing\n",
    "null_counts_sorted = null_counts.sort_values(\"null_percentage\", ascending=False)\n",
    "\n",
    "display(null_counts_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bd9b69-17bb-4239-928a-c6644b4daa6e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Status\":117},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758375926800}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. For each numeric column, are there extreme outliers (e.g., values beyond 1.5×IQR or z-score > 3)? (Outlier detection)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "numeric_cols = df_clean_final.columns[1:]  # exclude 'class' if it's first\n",
    "\n",
    "outlier_summary = []\n",
    "total_rows = df_clean_final.count()\n",
    "\n",
    "for c in numeric_cols:\n",
    "    try:\n",
    "        # Compute quartiles\n",
    "        q1, q3 = df_clean_final.approxQuantile(c, [0.25, 0.75], 0.01)\n",
    "        if q1 is None or q3 is None:\n",
    "            outlier_summary.append((c, None, None, \"No quartiles\"))\n",
    "            continue\n",
    "\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            outlier_summary.append((c, 0, 0.0, \"Constant column\"))\n",
    "            continue\n",
    "\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "        # Count outliers\n",
    "        count_outliers = df_clean_final.filter(\n",
    "            (col(c) < lower_bound) | (col(c) > upper_bound)\n",
    "        ).count()\n",
    "\n",
    "        percent_outliers = round(count_outliers / total_rows * 100, 2)\n",
    "\n",
    "        outlier_summary.append((c, count_outliers, percent_outliers, \"OK\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        outlier_summary.append((c, None, None, f\"Error: {str(e)}\"))\n",
    "\n",
    "# Convert to Pandas\n",
    "outlier_df = pd.DataFrame(\n",
    "    outlier_summary,\n",
    "    columns=[\"Feature\", \"Outlier_Count\", \"Outlier_Percentage\", \"Status\"]\n",
    ").sort_values(\"Outlier_Percentage\", ascending=False)\n",
    "\n",
    "display(outlier_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d4e9e0-a975-4fbb-9714-d1ce3db3e713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Outlier Detection – Observation  \n",
    "\n",
    "We applied **two methods** for outlier detection across all numeric columns:  \n",
    "\n",
    "- **IQR method** (1.5 × IQR rule)  \n",
    "- **Z-score method** (> 3 standard deviations)  \n",
    "\n",
    "Both approaches consistently showed either **0% detected outliers** or produced constant values across many features. This suggests that:  \n",
    "\n",
    "- The sensor readings are very stable within a narrow operating range.  \n",
    "- There is low variability across features, making extreme deviations unlikely.  \n",
    "- The dataset appears clean and consistent, with no significant anomalies or faulty sensor values.  \n",
    "\n",
    "**Conclusion:** The APS dataset does not exhibit meaningful outliers at the column level, which aligns with expectations for structured sensor data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984681c0-84fa-41cc-a346-4e1b9a565aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. How does the distribution (histogram) of each sensor column look? Is it normal, uniform, or skewed? (Pattern detection per sensor)\n",
    "\n",
    "from pyspark.sql.functions import skewness\n",
    "import pandas as pd\n",
    "\n",
    "numeric_cols = df_clean_final.columns[1:]  # exclude 'class'\n",
    "\n",
    "# Compute skewness for each column\n",
    "skew_data = []\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    sk_val = df_clean_final.select(skewness(col(col_name))).first()[0]\n",
    "    \n",
    "    # Determine distribution type with direction\n",
    "    if sk_val is None:\n",
    "        dist_type = \"Unknown\"\n",
    "    elif sk_val > 1:\n",
    "        dist_type = \"Highly Right-Skewed\"\n",
    "    elif 0.5 < sk_val <= 1:\n",
    "        dist_type = \"Moderately Right-Skewed\"\n",
    "    elif -0.5 <= sk_val <= 0.5:\n",
    "        dist_type = \"Approximately Symmetric\"\n",
    "    elif -1 <= sk_val < -0.5:\n",
    "        dist_type = \"Moderately Left-Skewed\"\n",
    "    else:  # sk_val < -1\n",
    "        dist_type = \"Highly Left-Skewed\"\n",
    "    \n",
    "    skew_data.append((col_name, sk_val, dist_type))\n",
    "\n",
    "# Convert to Pandas for display\n",
    "dist_df = pd.DataFrame(skew_data, columns=[\"Feature\", \"Skewness\", \"Distribution_Type\"])\n",
    "\n",
    "display(dist_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05022e4-ac91-43a4-950e-616c02a6543a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. How does each sensor column differ between class=1 vs. class=0 (group-wise means/medians)? (Feature discrimination check)\n",
    "\n",
    "from pyspark.sql.functions import col, mean\n",
    "import pandas as pd\n",
    "\n",
    "numeric_cols = df_clean_final.columns[1:]  # exclude 'class'\n",
    "classes = [0, 1]  # class values\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through classes\n",
    "for cls in classes:\n",
    "    df_class = df_clean_final.filter(col(\"class\") == cls)\n",
    "    total_rows = df_class.count()\n",
    "    \n",
    "    for c in numeric_cols:\n",
    "        # Mean\n",
    "        mean_val = df_class.select(mean(col(c))).first()[0]\n",
    "        \n",
    "        # Median using approxQuantile\n",
    "        median_val = df_class.approxQuantile(c, [0.5], 0.01)[0]  # 1% error tolerance\n",
    "        \n",
    "        results.append((c, cls, mean_val, median_val))\n",
    "\n",
    "# Convert to Pandas for easier display\n",
    "eda7_df = pd.DataFrame(results, columns=[\"Feature\", \"Class\", \"Mean\", \"Median\"])\n",
    "\n",
    "# Optional: sort by Feature and Class\n",
    "eda7_df = eda7_df.sort_values([\"Feature\", \"Class\"])\n",
    "\n",
    "display(eda7_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d9e85a5-331d-4416-a4bd-b39b8c4a0bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Business Problems \n",
    "\n",
    "### 1. Part-to-Whole Analysis – Component Contribution\n",
    "\n",
    "**Question:** Which sensors or systems contribute the most to overall abnormal readings or failure indicators?\n",
    "\n",
    "**Business Value:** Helps prioritize maintenance or redesign for high-impact components. Fleet managers can focus on the sensors/parts that “matter most” for vehicle reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e073c5b1-e1b7-409c-90a1-4988419eb188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, stddev\n",
    "import pandas as pd\n",
    "\n",
    "numeric_cols = df_clean_final.columns[1:]  # exclude 'class'\n",
    "\n",
    "# Compute mean and stddev for each sensor\n",
    "summary_stats = df_clean_final.select([\n",
    "    mean(col(c)).alias(c + \"_mean\") for c in numeric_cols\n",
    "] + [\n",
    "    stddev(col(c)).alias(c + \"_std\") for c in numeric_cols\n",
    "]).collect()[0]\n",
    "\n",
    "# Identify abnormal readings (above mean + 2*stddev)\n",
    "abnormal_counts = []\n",
    "total_abnormal = 0\n",
    "\n",
    "for c in numeric_cols:\n",
    "    mean_val = summary_stats[c + \"_mean\"]\n",
    "    std_val = summary_stats[c + \"_std\"]\n",
    "    \n",
    "    if std_val is not None:\n",
    "        upper_threshold = mean_val + 2 * std_val\n",
    "        count = df_clean_final.filter(col(c) > upper_threshold).count()\n",
    "    else:\n",
    "        count = 0\n",
    "    \n",
    "    abnormal_counts.append((c, count))\n",
    "    total_abnormal += count\n",
    "\n",
    "# Calculate contribution % without rounding and add a flag\n",
    "abnormal_summary = []\n",
    "for sensor, count in abnormal_counts:\n",
    "    if total_abnormal > 0:\n",
    "        percent = (count / total_abnormal) * 100\n",
    "    else:\n",
    "        percent = 0.0\n",
    "    flag = \"Yes\" if count > 0 else \"No\"\n",
    "    abnormal_summary.append((sensor, count, percent, flag))\n",
    "\n",
    "# Convert to Pandas for display\n",
    "abnormal_df = pd.DataFrame(abnormal_summary, columns=[\"Sensor\", \"Abnormal_Count\", \"Contribution_Percentage\", \"Has_Abnormal\"])\n",
    "abnormal_df = abnormal_df.sort_values(\"Contribution_Percentage\", ascending=False)\n",
    "\n",
    "display(abnormal_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d038f3aa-00ce-4ad6-9bcb-f23d74907480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Change Over Time / Trend Analysis\n",
    "\n",
    "**Question:** How do key sensor readings evolve over time for the fleet? Are there trends indicating degradation, wear, or unusual operational patterns?\n",
    "\n",
    "**Business Value:** Identifies early signs of system wear, abnormal usage patterns, or performance drift across vehicles. Enables proactive maintenance and reduces unexpected breakdowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98669f17-49f1-4344-89ec-71c18a815c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Add a row index column\n",
    "df_indexed = df_clean_final.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "\n",
    "# Step 2: Get total rows and quartile size\n",
    "total_rows = df_indexed.count()\n",
    "quartile_size = total_rows // 4\n",
    "\n",
    "trend_summary = []\n",
    "\n",
    "# Step 3: Loop through numeric columns\n",
    "for c in numeric_cols:\n",
    "    quartile_means = []\n",
    "    for q in range(4):\n",
    "        start_idx = q * quartile_size\n",
    "        end_idx = (q + 1) * quartile_size if q < 3 else total_rows\n",
    "        \n",
    "        mean_val = (\n",
    "            df_indexed.filter(\n",
    "                (F.col(\"row_idx\") >= start_idx) & (F.col(\"row_idx\") < end_idx)\n",
    "            )\n",
    "            .select(F.mean(F.col(c)))\n",
    "            .first()[0]\n",
    "        )\n",
    "        quartile_means.append(mean_val)\n",
    "    \n",
    "    # --- Trend flag ---\n",
    "    if all(m == quartile_means[0] for m in quartile_means):\n",
    "        trend = \"stable\"\n",
    "    elif quartile_means == sorted(quartile_means):\n",
    "        trend = \"increasing\"\n",
    "    elif quartile_means == sorted(quartile_means, reverse=True):\n",
    "        trend = \"decreasing\"\n",
    "    else:\n",
    "        trend = \"fluctuating\"\n",
    "    \n",
    "    trend_summary.append((c, *quartile_means, trend))\n",
    "\n",
    "# Step 4: Convert to Pandas DataFrame for display\n",
    "trend_df = pd.DataFrame(\n",
    "    trend_summary,\n",
    "    columns=[\"Sensor\", \"Q1_mean\", \"Q2_mean\", \"Q3_mean\", \"Q4_mean\", \"Trend_Flag\"]\n",
    ")\n",
    "\n",
    "display(trend_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f762531-988c-48ca-8c1f-d8cdd4813b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reporting / Aggregated Summaries\n",
    "\n",
    "**Question:** How can we summarize sensor behavior across vehicles or time periods to quickly identify anomalies or hotspots?\n",
    "\n",
    "**Business Value:** Creates a dashboard-like overview for management or maintenance teams. Highlights operational risk areas and supports quick, informed decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e80d7714-6f7a-4e38-b659-4cc5fe380eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, min as _min, max as _max, mean as _mean, stddev as _std, expr\n",
    "import pandas as pd\n",
    "\n",
    "# Numeric columns (exclude 'class')\n",
    "numeric_cols = df_clean_final.columns[1:]\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Compute Summary Stats\n",
    "# --------------------------\n",
    "summary_list = []\n",
    "for c in numeric_cols:\n",
    "    stats = df_clean_final.select(\n",
    "        _min(col(c)).alias(\"Min\"),\n",
    "        _max(col(c)).alias(\"Max\"),\n",
    "        _mean(col(c)).alias(\"Mean\"),\n",
    "        _std(col(c)).alias(\"StdDev\"),\n",
    "        expr(f'percentile_approx({c}, 0.5)').alias(\"Median\")\n",
    "    ).collect()[0]\n",
    "    summary_list.append((c, stats[\"Min\"], stats[\"Max\"], stats[\"Mean\"], stats[\"Median\"], stats[\"StdDev\"]))\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list, columns=[\"Sensor\", \"Min\", \"Max\", \"Mean\", \"Median\", \"StdDev\"])\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Abnormal readings (Mean + 2*StdDev)\n",
    "# --------------------------\n",
    "abnormal_list = []\n",
    "total_abnormal_all = 0\n",
    "\n",
    "for c in numeric_cols:\n",
    "    mean_val = summary_df.loc[summary_df[\"Sensor\"]==c, \"Mean\"].values[0]\n",
    "    std_val = summary_df.loc[summary_df[\"Sensor\"]==c, \"StdDev\"].values[0]\n",
    "    if std_val is not None:\n",
    "        upper_thresh = mean_val + 2*std_val\n",
    "        count = df_clean_final.filter(col(c) > upper_thresh).count()\n",
    "    else:\n",
    "        count = 0\n",
    "    abnormal_list.append((c, count))\n",
    "    total_abnormal_all += count\n",
    "\n",
    "abnormal_df = pd.DataFrame(abnormal_list, columns=[\"Sensor\", \"Abnormal_Count\"])\n",
    "abnormal_df[\"Contribution_Percentage\"] = abnormal_df[\"Abnormal_Count\"].apply(\n",
    "    lambda x: (x / total_abnormal_all * 100) if total_abnormal_all > 0 else 0\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Trend Flags (Quartile Means)\n",
    "# --------------------------\n",
    "df_indexed = df_clean_final.withColumn(\"row_idx\", F.monotonically_increasing_id())\n",
    "total_rows = df_indexed.count()\n",
    "quartile_size = total_rows // 4\n",
    "\n",
    "trend_list = []\n",
    "for c in numeric_cols:\n",
    "    quart_means = []\n",
    "    for q in range(4):\n",
    "        start_idx = q*quartile_size\n",
    "        end_idx = (q+1)*quartile_size if q<3 else total_rows\n",
    "        mean_val = df_indexed.filter(\n",
    "            (col(\"row_idx\")>=start_idx) & (col(\"row_idx\")<end_idx)\n",
    "        ).select(F.mean(col(c))).first()[0]\n",
    "        quart_means.append(mean_val)\n",
    "    \n",
    "    # Trend flag\n",
    "    if quart_means == sorted(quart_means):\n",
    "        trend = \"Increasing\"\n",
    "    elif quart_means == sorted(quart_means, reverse=True):\n",
    "        trend = \"Decreasing\"\n",
    "    else:\n",
    "        trend = \"Fluctuating\"\n",
    "    trend_list.append((c, trend))\n",
    "\n",
    "trend_df = pd.DataFrame(trend_list, columns=[\"Sensor\", \"Trend_Flag\"])\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Variability & Anomaly Flags\n",
    "# --------------------------\n",
    "std_mean = summary_df[\"StdDev\"].mean()\n",
    "summary_df[\"Variability_Flag\"] = summary_df[\"StdDev\"].apply(\n",
    "    lambda x: \"High Variability\" if x > 2*std_mean else \"Normal\"\n",
    ")\n",
    "abnormal_df[\"Anomaly_Flag\"] = abnormal_df[\"Contribution_Percentage\"].apply(\n",
    "    lambda x: \"High Anomalies\" if x > 5 else \"Normal\"\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Merge All into Final Report\n",
    "# --------------------------\n",
    "final_report = summary_df.merge(abnormal_df, on=\"Sensor\").merge(trend_df, on=\"Sensor\")\n",
    "\n",
    "# Display the polished report\n",
    "display(final_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ef8a53-e2d2-4aa7-8fce-0e2e958fbd71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_clean_final)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7735208481498900,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Scania Pyspark Project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}